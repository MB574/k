import os
import re
import json
import jieba
from dotenv import load_dotenv

if not os.getenv('GITHUB_ACTIONS'):
    load_dotenv()

from peewee import *
from model.mysql_models import (
    DB_MYSQL, Video, Document, SoraContent, FileTag, Tag, init_mysql
)

SYNC_TO_POSTGRES = os.getenv('SYNC_TO_POSTGRES', 'false').lower() == 'true'
BATCH_LIMIT = None
# ÂàùÂßãÂåñ MySQLÔºàÂøÖÈ°ªÂÖàÊâßË°åÔºâ
init_mysql()

# Â¶ÇÈúÄ PostgreSQLÔºåÂÜçÂØºÂÖ•Âπ∂ÂàùÂßãÂåñ
if SYNC_TO_POSTGRES:
    from model.pg_models import DB_PG, SoraContentPg, init_postgres
    from playhouse.shortcuts import model_to_dict
    init_postgres()

# Âêå‰πâËØçÂ≠óÂÖ∏
SYNONYM = {
    "ÊªëÈº†": "Èº†Ê†á",
    "Ëê§Âπï": "ÊòæÁ§∫Âô®",
    "Á¨îÁîµ": "Á¨îËÆ∞Êú¨",
}

def clean_text(original_string):
    target_strings = ["- Advertisement - No Guarantee", "- ÂπøÂëä - Êó†ÊãÖ‰øù"]
    for target in target_strings:
        pos = original_string.find(target)
        if pos != -1:
            original_string = original_string[:pos]

    replace_texts = [
        "Ê±ÇÊâìËµè", "Ê±ÇËµè", "ÂèØÈÄöËøá‰ª•‰∏ãÊñπÂºèËé∑ÂèñÊàñÂàÜ‰∫´Êñá‰ª∂",
        "ÁßÅËÅäÊ®°ÂºèÔºöÂ∞ÜÂê´ÊúâFile IDÁöÑÊñáÊú¨Áõ¥Êé•ÂèëÈÄÅÁªôÊú∫Âô®‰∫∫ @datapanbot Âç≥ÂèØËøõË°åÊñá‰ª∂Ëß£Êûê",
        "‚ë†ÁßÅËÅäÊ®°ÂºèÔºöÂ∞ÜÂê´ÊúâFile IDÁöÑÊñáÊú¨Áõ¥Êé•ÂèëÈÄÅÁªôÊú∫Âô®‰∫∫  Âç≥ÂèØËøõË°åÊñá‰ª∂Ëß£Êûê",
        "ÂçïÊú∫Â§çÂà∂Ôºö", "Êñá‰ª∂Ëß£Á†ÅÂô®:", "ÊÇ®ÁöÑÊñá‰ª∂Á†ÅÂ∑≤ÁîüÊàêÔºåÁÇπÂáªÂ§çÂà∂Ôºö",
        "ÊâπÈáèÂèëÈÄÅÁöÑÂ™í‰Ωì‰ª£Á†ÅÂ¶Ç‰∏ã:", "Ê≠§Êù°Â™í‰ΩìÂàÜ‰∫´link:",
        "Â•≥‰æÖÊêúÁ¥¢Ôºö@ seefilebot", "Ëß£Á†ÅÔºö@ MediaBK2bot",
        "Â¶ÇÊûúÊÇ®Âè™ÊòØÊÉ≥Â§á‰ªΩÔºåÂèëÈÄÅ /settings ÂèØ‰ª•ËÆæÁΩÆÂÖ≥Èó≠Ê≠§Êù°ÂõûÂ§çÊ∂àÊÅØ",
        "Â™í‰ΩìÂåÖÂ∑≤ÂàõÂª∫ÔºÅ", "Ê≠§Â™í‰Ωì‰ª£Á†Å‰∏∫:", "Êñá‰ª∂ÂêçÁß∞:", "ÂàÜ‰∫´ÈìæÊé•:", "|_SendToBeach_|",
        "Forbidden: bot was kicked from the supergroup chat",
        "Bad Request: chat_id is empty"
    ]
    for text in replace_texts:
        original_string = original_string.replace(text, '')

    original_string = re.sub(r"ÂàÜ‰∫´Ëá≥\d{4}-\d{2}-\d{2} \d{2}:\d{2} Âà∞ÊúüÂêéÊÇ®‰ªçÂèØÈáçÊñ∞ÂàÜ‰∫´", '', original_string)

    json_pattern = r'\{[^{}]*?"text"\s*:\s*"[^"]+"[^{}]*?\}'
    matches = re.findall(json_pattern, original_string)
    for match in matches:
        try:
            data = json.loads(match)
            if 'content' in data and isinstance(data['content'], str):
                original_string += f"\n{data['content']}"
        except json.JSONDecodeError:
            pass
        original_string = original_string.replace(match, '')

    wp_patterns = [r'https://t\.me/[^\s]+']
    for pattern in wp_patterns:
        original_string = re.sub(pattern, '', original_string)

    for pat in [
        r'LINK\s*\n[^\n]+#C\d+\s*\nOriginal:[^\n]*\n?',
        r'LINK\s*\n[^\n]+#C\d+\s*\nForwarded from:[^\n]*\n?',
        r'LINK\s*\n[^\n]*#C\d+\s*',
        r'Original caption:[^\n]*\n?'
    ]:
        original_string = re.sub(pat, '', original_string)

    original_string = re.sub(r'^\s*$', '', original_string, flags=re.MULTILINE)
    lines = original_string.split('\n')
    unique_lines = list(dict.fromkeys(lines))
    result_string = "\n".join(lines)

    for symbol in ['üîë', 'üíé']:
        result_string = result_string.replace(symbol, '\r\n' + symbol)

    return result_string[:1500] if len(result_string) > 1500 else result_string

def replace_synonym(text):
    for k, v in SYNONYM.items():
        text = text.replace(k, v)
    return text

def segment_text(text):
    text = replace_synonym(text)
    return " ".join(jieba.cut(text))

def fetch_tag_cn_for_file(file_unique_id):
    return [
        t.tag_cn for t in Tag.select()
        .join(FileTag, on=(FileTag.tag == Tag.tag))
        .where(FileTag.file_unique_id == file_unique_id)
        if t.tag_cn
    ]

def sync_to_postgres(record):
    if not SYNC_TO_POSTGRES:
        return

    from playhouse.shortcuts import model_to_dict
    model_data = model_to_dict(record, recurse=False)
    model_data["id"] = record.id  # ‚úÖ ‰∏ªÈîÆÊòæÂºèÂ∏¶ÂÖ•

    # ‚úÖ Âº∫Âà∂Â∏¶ÂÖ• idÔºåÁ°Æ‰øù‰∏ªÈîÆ‰∏ÄËá¥
    with DB_PG.atomic():
        try:
            existing = SoraContentPg.get(SoraContentPg.id == record.id)
            for k, v in model_data.items():
                setattr(existing, k, v)
            existing.save()
        except SoraContentPg.DoesNotExist:
            SoraContentPg.create(**model_data)


def process_documents():
    DB_MYSQL.connect()
    if SYNC_TO_POSTGRES:
        DB_PG.connect()

    for doc in Document.select().where((Document.kc_status.is_null(True)) | (Document.kc_status != 'updated')).limit(BATCH_LIMIT):
        if not doc.file_name and not doc.caption:
            doc.kc_status = 'updated'
            doc.save()
            continue

        content = clean_text(f"{doc.file_name or ''}\n{doc.caption or ''}")
        content_seg = segment_text(content)
        tag_cn_list = fetch_tag_cn_for_file(doc.file_unique_id)
        if tag_cn_list:
            content_seg += " " + " ".join(tag_cn_list)

        print(f"Processing {doc.file_unique_id}: {content_seg}")

        if doc.kc_id:
            try:
                kw = SoraContent.get_by_id(doc.kc_id)
                kw.source_id = doc.file_unique_id
                kw.content = content
                kw.content_seg = content_seg
                kw.file_size = doc.file_size
                kw.save()
            except SoraContent.DoesNotExist:
                kw = SoraContent.create(
                    source_id=doc.file_unique_id, 
                    type='d', 
                    content=content, 
                    content_seg=content_seg,
                    file_size = doc.file_size
                    )
                doc.kc_id = kw.id
        else:
            kw = SoraContent.create(
                source_id=doc.file_unique_id, type='d', content=content, content_seg=content_seg,file_size = doc.file_size)
            doc.kc_id = kw.id

        doc.kc_status = 'updated'
        doc.save()

       
        if SYNC_TO_POSTGRES and kw.id:     
            sync_to_postgres(kw)

    DB_MYSQL.close()
    if SYNC_TO_POSTGRES:
        DB_PG.close()


def process_videos():
    DB_MYSQL.connect()
    if SYNC_TO_POSTGRES:
        DB_PG.connect()

    for doc in Video.select().where((Video.kc_status.is_null(True)) | (Video.kc_status != 'updated')).limit(BATCH_LIMIT):
        if not doc.file_name and not doc.caption:
            doc.kc_status = 'updated'
            doc.save()
            continue

        content = clean_text(f"{doc.file_name or ''}\n{doc.caption or ''}")
        content_seg = segment_text(content)
        tag_cn_list = fetch_tag_cn_for_file(doc.file_unique_id)
        if tag_cn_list:
            content_seg += " " + " ".join(tag_cn_list)

        print(f"Processing {doc.file_unique_id}: {content_seg}")

        if doc.kc_id:
            try:
                kw = SoraContent.get_by_id(doc.kc_id)
                kw.source_id = doc.file_unique_id
                kw.content = content
                kw.content_seg = content_seg
                kw.file_size = doc.file_size
                kw.duration = doc.duration
                kw.save()
            except SoraContent.DoesNotExist:
                kw = SoraContent.create(
                    source_id=doc.file_unique_id, 
                    type='v', 
                    content=content, 
                    content_seg=content_seg,
                    file_size = doc.file_size,
                    duration = doc.duration
                    )
                doc.kc_id = kw.id
        else:
            kw = SoraContent.create(
                source_id=doc.file_unique_id, 
                type='v', 
                content=content, 
                content_seg=content_seg,
                file_size = doc.file_size,
                duration = doc.duration
                )
            doc.kc_id = kw.id

        doc.kc_status = 'updated'
        doc.save()

       
        if SYNC_TO_POSTGRES and kw.id:     
            sync_to_postgres(kw)

    DB_MYSQL.close()
    if SYNC_TO_POSTGRES:
        DB_PG.close()

if __name__ == "__main__":
    process_documents()
    process_videos()
